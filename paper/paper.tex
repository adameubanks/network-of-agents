\documentclass[conference]{IEEEtran}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{tabularx}
\usepackage{array}
\usepackage{url}

% Model name macros and placeholders (restricted to three models)
\newcommand{\modelFiveNano}{gpt-5-nano}
\newcommand{\modelFiveMini}{gpt-5-mini}
\newcommand{\modelGrokMini}{grok-mini}
\newcommand{\Placeholder}{--}
\newcommand{\modelsCompared}{\modelFiveNano, \modelFiveMini, \modelGrokMini}

\title{Algorithmic Fidelity of Large Language Models in Multi-Agent Systems}

\author{%
  Adam Eubanks \\
  Brigham Young University \\
  \texttt{adameuba@byu.edu}
  \and
  Caelen Miller \\
  Brigham Young University \\
  \texttt{cm725@byu.edu}
  \and
  Sean Warnick \\
  Brigham Young University \\
  \texttt{sean@cs.byu.edu}
}

\begin{document}

\maketitle

\begin{abstract}
We evaluate whether large language model (LLM) agents reproduce classical DeGroot opinion dynamics in multi-agent settings. We define algorithmic fidelity as the extent to which an LLM-induced opinion-dynamics operator reproduces the fixed points and update trajectories of the corresponding mathematical DeGroot operator (without any LLM), after affine calibration of the observation layer. Our protocol instantiates an A-vs-B axis, iterating generate $\rightarrow$ rate $\rightarrow$ update; we fit calibration (\(\alpha_c,\beta_c\), RMSE) for the measurement layer and compare induced dynamics to pure DeGroot using divergence metrics (RMSE/MAE/correlation), fixed-point error, and order-reversal symmetry tests. We apply the protocol to politically salient and apolitical topics spanning favorable, unfavorable, and divided human priors, and include comparisons to polling baselines. Our contributions are: (1) a formalization of algorithmic fidelity for LLM--MAS against the pure DeGroot operator, (2) metrics and estimators for calibration and dynamics layers, and (3) a practical evaluation protocol for topic-level assessment.\end{abstract}

\section{Introduction}
Large language models (LLMs) enable partially automated implementations of opinion-controller experiments; their fidelity to classical models remains an open question. Recent work by DeBuse and Warnick (2024) \cite{debuse2024study} showed that LLMs can translate quantified commands from opinion controllers into usable content, bridging mathematical control and social media deployment.

We study whether these automated agents preserve the mathematical dynamics that make opinion control predictable. \textbf{Algorithmic fidelity} is the extent to which an LLM-induced opinion-dynamics operator reproduces the fixed points and update trajectories of the corresponding mathematical DeGroot operator (without any LLM), after affine calibration of the observation layer. This lens is distinct from content quality; it evaluates whether the induced dynamical system matches operator-theoretic predictions (update rules, equilibria, and symmetry).

\textbf{Research Questions:} (1) Do LLM agents reproduce DeGroot-like behavior in aggregate? (2) Do they maintain symmetry under topic order reversal? (3) How large are the deviations in bias, fixed-point error, and variance?

\textbf{Contributions:}
\begin{itemize}
  \item Formalize algorithmic fidelity for LLM--MAS against the pure DeGroot operator.
  \item Propose calibration-layer metrics (\(\alpha_c,\beta_c\), RMSE) and dynamics-layer divergence metrics (RMSE/MAE/correlation, fixed-point error, symmetry tests).
  \item Demonstrate an evaluation protocol on politically salient and apolitical topics.
\end{itemize}
We next describe the protocol and metrics (Methods) and defer exact prompts and parsing details to Appendix~\ref{app:prompts_parsing}.

\section{Related Work}
\textbf{Automated Opinion Controllers:} The foundational work by DeBuse and Warnick (2024) \cite{debuse2024study} established the practical feasibility of using Large Language Models as automated agents in opinion dynamics. Their study introduced three archetypal controller types (stubborn, popular, and strategic agents) and demonstrated that LLMs can effectively translate numerical control signals into nuanced social media content. Critically, they showed that current generative AI technologies can both infer opinions from social media posts and generate appropriate responses based on quantified commands, making the implementation of automated social influence systems remarkably straightforward. However, their work focused on the \textit{capability} of LLMs to implement opinion controllers rather than their \textit{fidelity} to mathematical models.

\textbf{Multi-Agent Opinion Dynamics:} The DeGroot model provides the mathematical foundation for understanding opinion evolution in multi-agent systems \cite{degroot1974reaching,proskurnikov2017tutorial}. This model assumes linear update rules and has been extensively studied in control theory, social psychology, and multi-agent systems research. The linear DeGroot model remains a workhorse for mathematical analysis, with extensive results on convergence and network effects.

\textbf{LLM-based Multi-Agent Systems:} Recent work has explored using LLMs as agents in various multi-agent simulation contexts. Park et al. (2023) introduced "Generative Agents" that exhibit human-like behavior in virtual environments, while works from NeurIPS and ICLR workshops have explored LLM agents in social simulations. Parallel efforts study LLMs as evaluators, e.g., MT-Bench and G-Eval \cite{zheng2023judging,liu2023geval}, and propose multi-judge frameworks to reduce single-judge bias \cite{juries2024}. However, most studies focus on emergent behavior and task performance rather than algorithmic fidelity to mathematical models. The multi-agent systems community has shown increasing interest in LLM-based agents, but systematic evaluation of their mathematical consistency remains underexplored.

\textbf{Algorithmic Fidelity in AI Systems:} The concept of algorithmic fidelity (measuring how well computational agents reproduce expected mathematical behavior) has been explored in various contexts, including neural network approximation theory and AI safety evaluation. However, this concept has not been systematically applied to multi-agent opinion dynamics, representing a critical gap in the literature. Our work extends this concept to the domain of automated social influence systems, where fidelity to mathematical models is crucial for reliable deployment. We connect to the broader framing of algorithmic fidelity as discussed in Silicon Sampling \cite{wingate2024silicon}.

\textbf{LLM Bias and Alignment:} Extensive research has documented systematic biases in LLMs, including gender, racial, and political biases. However, the impact of these biases on multi-agent opinion dynamics and their interaction with network effects remains largely unexplored. Our work provides the first systematic analysis of how LLM biases manifest in multi-agent social dynamics, building on DeBuse and Warnick's framework to understand the limitations of automated opinion controllers.

\textbf{Ethical Considerations in Automated Social Influence:} DeBuse and Warnick (2024) \cite{debuse2024study} highlighted the ethical implications of automated opinion controllers, drawing on frameworks from biomedical research (Belmont Report) and cybersecurity (Menlo Report). They emphasized the need for responsible development and deployment of automated social influence systems. Our work contributes to this discussion by providing empirical evidence of algorithmic fidelity failures that could undermine the reliability and predictability of such systems.

\section{Mathematical Framework}
\subsection{Classical DeGroot Model}
In the classical DeGroot model, each agent $i$ has an opinion $x_i^{(t)} \in [0,1]$ at time $t$. The opinion update rule is:
\[
x_i^{(t+1)} = \sum_{j \in \mathcal{N}(i)} w_{ij} x_j^{(t)}
\]
where $\mathcal{N}(i)$ is the set of neighbors of agent $i$, and $w_{ij}$ are non-negative weights that sum to 1 for each agent.

\subsection{LLM-based Opinion Dynamics}
We extend the DeGroot model to include LLM-based text generation and interpretation. The key innovation is using an A vs B axis approach, where each topic is framed as a comparison between two options. This allows us to better define which side the LLM favors rather than just measuring how much it likes a given topic.

\textbf{Text Generation Operator $G_\theta$:} Given an opinion value $x_i^{(t)} \in [-1,1]$ and context $\mathcal{C}_i^{(t)}$ (consisting of recent posts from neighboring agents), the LLM generates a text post:
\[
p_i^{(t)} = G_\theta(x_i^{(t)}, \mathcal{C}_i^{(t)})
\]
where $\mathcal{C}_i^{(t)} = \{p_j^{(t-k)} : j \in \mathcal{N}(i), k \in \{1,2,\ldots,6\}\}$ represents the set of up to 6 most recent posts from agent $i$'s neighbors. Note that the context includes posts but not their numerical ratings.

\textbf{Rating Operator $R_\theta$:} Given a text post, the LLM returns a numeric rating:
\[
r_{j \to i}^{(t)} = R_\theta(p_j^{(t)}) \in [-1,1]
\]

\textbf{Scale Conversion:} Since the DeGroot model operates on $[0,1]$ while our LLM agents use $[-1,1]$, we convert between scales:
\[
x_{\text{math}} = \frac{x_{\text{agent}} + 1}{2}, \quad x_{\text{agent}} = 2x_{\text{math}} - 1
\]

\textbf{Update Function $f$:} The opinion update combines ratings from neighbors:
\[
x_i^{(t+1)} = f(\{r_{j \to i}^{(t)}\}_{j \in \mathcal{N}(i)})
\]

\subsection{Algorithmic Fidelity}
We view the overall system as an operator $T$ over opinions. In DeGroot, $T$ is linear and defined by the influence matrix. The LLM-based pipeline induces an operator $T_{\mathrm{LLM}}$ via text generation, rating, and updates. Perfect algorithmic fidelity means $T_{\mathrm{LLM}}$ reproduces $T$ under the same conditions. We decompose fidelity into: (i) a \emph{measurement layer} (calibration) mapping intended opinions $o$ to LLM ratings $r$, and (ii) a \emph{dynamics layer} comparing the induced update behavior to DeGroot.

\subsection{Calibration Framework}
Calibration quantifies the measurement layer $o\!\to\!r$, i.e., how the LLM's numeric rating $r$ of a post matches the intended opinion $o$ used to generate it. Let $o \in [0,1]$ denote the intended opinion (obtained by mapping the agent-domain value from $[-1,1]$ via $(x+1)/2$), and let $r \in [0,1]$ be the LLM's numeric rating parsed from its interpretation of the post. Define the calibration function $g(o) = \mathbb{E}[r\mid o]$; perfect calibration corresponds to $g(o) = o$.

\textit{Data construction.} Opinions and ratings are logged in $[-1,1]$ and mapped to $[0,1]$ via $(x+1)/2$ and clipped to $[0,1]$. For each agent and timestep $t$, we pair the rating at time $t$ with the intended opinion from time $t-1$ (storage occurs after the update). Metrics are computed per topic and summarized across topics.

\textbf{Centered affine model (on $[0,1]$).} We fit
\[
r - 0.5 = \alpha_c + \beta_c\,(o - 0.5).
\]
Here $\alpha_c = g(0.5) - 0.5$ is the neutrality shift (systematic tilt at the center), and $\beta_c$ is the sensitivity (compression if $\beta_c<1$, amplification if $\beta_c>1$). We report $(\alpha_c,\beta_c)$, centered $R^2$, and the calibration error $\mathrm{RMSE}=\sqrt{\mathbb{E}[(r-o)^2]}$.

\section{Methods}
\subsection{System Architecture}
Our experimental system consists of three main components:

\textbf{1. Agent System:} Each agent $i$ maintains a current opinion $x_i^{(t)} \in [-1,1]$ and can generate posts and interpret others' posts.

\textbf{2. LLM Client:} Handles communication with GPT-5 and Grok via the LiteLLM library, managing post generation and rating tasks.

\textbf{3. Simulation Controller:} Orchestrates the multi-agent simulation, managing opinion updates and network dynamics.

\subsection{Prompt Design and Opinion Scale}
We use an A vs B axis framing to stabilize expression and enable symmetry tests. Exact post generation and rating prompts, parsing rules, and examples are provided in Appendix~\ref{app:prompts_parsing}. Opinions use a continuous scale from $-1$ to $1$, mapped to $[0, 1]$ for DeGroot comparisons.

\subsection{Simulation Protocol}
The simulation follows this exact protocol for each timestep $t$:

\textbf{Step 1: Post Generation}
\begin{enumerate}
  \item For each connected agent $i$ (degree $> 0$), generate a post using the post generation prompt
  \item Include up to 6 most recent neighbor posts as context (truncated to 220 characters each)
  \item Prefix each generated post with "Agent {i}:" for identification
\end{enumerate}

\textbf{Step 2: Post Rating}
\begin{enumerate}
  \item For each agent $i$ and each neighbor $j$ (where $A_{ij} = 1$), agent $i$ rates agent $j$'s post
  \item Use the rating prompt to extract a numeric value in $[-1, 1]$
  \item Parse the response using regex pattern matching to extract the last numeric value
  \item Clamp values to $[-1, 1]$ range
\end{enumerate}

\textbf{Step 3: Opinion Update}
\begin{enumerate}
  \item For each agent $i$, compute the mean rating received from neighbors:
  \[
  \bar{r}_i^{(t)} = \frac{1}{|\mathcal{N}(i)|} \sum_{j \in \mathcal{N}(i)} r_{j \to i}^{(t)}
  \]
  \item Convert to math domain $[0,1]$: $x_i^{(t)} = \frac{\bar{r}_i^{(t)} + 1}{2}$
  \item Apply DeGroot update: $x_i^{(t+1)} = \sum_{j} w_{ij} x_j^{(t)}$
  \item Convert back to agent domain $[-1,1]$: $x_i^{(t+1)} = 2x_i^{(t+1)} - 1$
\end{enumerate}

\textbf{Step 4: Pure DeGroot Comparison}
\begin{enumerate}
  \item Calculate what pure DeGroot would produce using the same initial conditions
  \item Compute divergence metrics (MAE, RMSE, correlation) between LLM and pure DeGroot results
\end{enumerate}

\subsection{Experimental Parameters}
\textbf{Simulation Parameters:}
\begin{itemize}
  \item \textbf{Agents:} 50 agents per simulation
  \item \textbf{Timesteps:} 50 iterations
  \item \textbf{Network:} Sparse random graph (5\% connectivity, 67 edges out of 1225 possible); same graph across runs via fixed seed (=42)
  \item \textbf{Temperature:} Provider default; held constant across trials
  \item \textbf{Opinion Scale:} $[-1, 1]$ for LLM, converted to $[0, 1]$ for DeGroot comparison

\end{itemize}

\subsection{Models Compared}
We report results for the following models under an identical protocol and prompts: \modelsCompared.

\textbf{Network Topology Considerations:} The sparse random graph topology (5\% connectivity) represents a more realistic social network structure compared to complete graphs, where each agent is connected to only a small subset of other agents. This topology choice has important implications: (1) \textit{Local Influence:} Agents are primarily influenced by their immediate neighbors rather than the entire population, (2) \textit{Information Diffusion:} Opinion changes propagate through the network more gradually, and (3) \textit{Convergence Behavior:} The DeGroot model on sparse graphs may exhibit different convergence properties compared to complete graphs. The average degree of 2.7 means most agents interact with only 2-3 other agents per timestep, creating a more constrained information environment that may amplify the effects of LLM biases.

\subsection{Experimental Controls}
To support repeatability, we held core implementation parameters fixed across all trials:
\begin{itemize}
  \item \textbf{Random Seed:} A fixed seed was used to generate the initial network topology and stored with run artifacts.
  \item \textbf{Decoding Parameters:} Temperature and related sampling settings were left at provider defaults and not varied; the model identifier and prompt templates were fixed across runs.
  \item \textbf{Execution Order:} Agent update order was fixed each timestep, and calls were executed in isolated sessions with no cross-run state.
\end{itemize}

\subsection{Topics and Polling Baselines}
We select topics spanning political and apolitical issues with human preferences that are strongly favorable, strongly unfavorable, or approximately divided. This provides a litmus test to detect LLM biases that do not match human priors and to compare against the pure DeGroot path.

\textbf{Political \& Social Issues}

\textit{Strongly Favorable View}

\begin{enumerate}
  \item \textbf{Immigration: Overall Impact on the Country}
  \\
  A vs B framing: ``On the whole, do you think immigration is a good thing or a bad thing for this country (United States) today?''
  \\
  Poll result: 79\% say good thing vs 17\% bad thing. Source: \cite{gallup_immigration_2025}.
\end{enumerate}

\textit{Divided View --- Near 50/50 Split}

\begin{enumerate}
  \setcounter{enumi}{1}
  \item \textbf{Environment vs Economic Growth}
  \\
  A vs B framing: Prioritize environmental protection even if growth is curbed vs prioritize economic growth even if the environment suffers to some extent.
  \\
  Poll result: 52\% environment vs 43\% growth. Source: \cite{gallup_env_growth_2023}.

  \item \textbf{Corporate Activism: Company Statements}
  \\
  A vs B framing: It is important vs not important for companies to make statements about political/social issues.
  \\
  Poll result: 50\% important vs 50\% not important. Source: \cite{pew_corp_statements_2025}.

  \item \textbf{Gun Policy: Safety vs Risk}
  \\
  A vs B framing: Gun ownership increases safety vs reduces safety.
  \\
  Poll result: 49\% increases safety vs 49\% reduces safety. Source: \cite{pew_gun_safety_2023}.
\end{enumerate}

\textit{Strongly Unfavorable View}

\begin{enumerate}
  \setcounter{enumi}{4}
  \item \textbf{Social Media and Democracy}
  \\
  A vs B framing: Social media has been good vs bad for democracy in the U.S.
  \\
  Poll result: 34\% good vs 64\% bad. Source: \cite{pew_social_media_democracy_us_2024}.
\end{enumerate}

\textbf{Apolitical \& Cultural Debates}

\textit{Strongly Favorable View}

\begin{enumerate}
  \setcounter{enumi}{5}
  \item \textbf{Toilet Paper Orientation}
  \\
  A vs B framing: Over vs under the roll.
  \\
  Poll result: 59\% over vs 14\% under (21\% no preference). Source: \cite{yougov_toilet_paper_2022}.
\end{enumerate}

\textit{Divided View --- Near 50/50 Split}

\begin{enumerate}
  \setcounter{enumi}{6}
  \item \textbf{Is a Hot Dog a Sandwich?}
  \\
  A vs B framing: Yes vs No.
  \\
  Poll result: 41\% yes vs 49\% no. Source: \cite{yougov_hotdog_2023}.

  \item \textbf{Child-Free Weddings}
  \\
  A vs B framing: Always/usually appropriate vs always/usually inappropriate.
  \\
  Poll result: 45\% appropriate vs 40\% inappropriate. Source: \cite{yougov_wedding_etiquette_2023}.
\end{enumerate}

\textit{Strongly Unfavorable View}

\begin{enumerate}
  \setcounter{enumi}{8}
  \item \textbf{Snapping at a Waiter}
  \\
  A vs B framing: Acceptable vs unacceptable to snap fingers to get attention.
  \\
  Poll result: 11\% acceptable vs 81\% unacceptable. Source: \cite{yougov_restaurant_etiquette_2024}.

  \item \textbf{Moral Acceptability of Human Cloning}
  \\
  A vs B framing: Morally acceptable vs morally wrong.
  \\
  Poll result: 8\% acceptable vs 87\% wrong. Source: \cite{gallup_cloning_2025}.
\end{enumerate}

\textbf{Rationale.} We include politically salient and apolitical topics with human priors that are favorable, unfavorable, and divided to test whether LLM dynamics preserve expected symmetries and baselines. This lets us detect biases or asymmetries misaligned with human presumptions and to compare against the pure DeGroot path.

\section{Results}
-

\section{Discussion}
-

\section{Limitations \& Future Work}
\textbf{Limitations:} Key factors affecting generalizability include: (1) \textit{Model/Provider Scope:} Results reflect GPT-5 and Grok under specific provider settings that may drift over time, (2) \textit{Prompt/Scale Choices:} A single prompt template and a $[-1,1]\leftrightarrow[0,1]$ mapping were used; alternative framings may change outcomes, (3) \textit{Network Topology:} A single sparse random graph (5\% connectivity; seed=42) was used, (4) \textit{Determinism:} Fixed decoding settings; effects of stochastic sampling were not explored, (5) \textit{Generator--Rater Coupling:} Ratings were produced by LLM raters without human validation, and (6) \textit{Topic Set/Refusals:} A finite topic set with potential refusal filters may bias observed dynamics.

\textbf{Reproducibility:} All code, data, and experimental configurations are available at [GitHub repository]. The simulation framework is implemented in Python using the LiteLLM library, and all prompts, parameters, and results are documented. Raw data includes all generated posts, ratings, and opinion trajectories for each simulation. We follow emerging guidance on repeat evaluations and uncertainty reporting for LLM benchmarks \cite{reproLLMuncertainty2024}.

\textbf{Future Work:}
\begin{itemize}
  \item \textbf{Multi-Model Replication:} Track provider/version and compare across additional models under identical protocols
  \item \textbf{Human Baselines:} Incorporate human raters to validate the measurement layer and assess agreement
  \item \textbf{Calibration Protocols:} Develop robust affine calibration with anchors and pre/post checks across topics
  \item \textbf{Topology/Temperature:} Evaluate alternative network topologies and controlled stochastic decoding
  \item \textbf{Refusal Handling:} Detect and mitigate refusals and safety-filter artifacts that skew dynamics
\end{itemize}

\section{Conclusion}
-


\bibliographystyle{IEEEtran}
\bibliography{references}

\appendix
\section{Prompt Templates and Parsing}
\label{app:prompts_parsing}
This appendix provides the exact post generation and rating prompts, along with parsing details and examples used in the experiments.

\textbf{Post Generation Prompt:}
\begin{quote}
\small
Write a short, social-media style post (1--3 sentences, $\leq$320 characters) in first person about \texttt{\{a\}} vs \texttt{\{b\}}. Your current opinion: \texttt{\{self.current\_opinion:.3f\}} ($-1$ favors \texttt{\{a\}}, $1$ favors \texttt{\{b\}}).

If you see other agents' posts, you may respond to 1--2 of them by name (e.g., Agent 7), briefly quote or paraphrase, and agree, disagree, or ask a question. Prose only, no numeric score.
\end{quote}

\textbf{Rating Prompt:}
\begin{quote}
\small
Rate the post on the axis \texttt{\{a\}} ($-1$) $\leftrightarrow$ \texttt{\{b\}} ($1$). Respond with only one number in [$-1.000$, $1.000$] on its own line. Use $0.000$ if neutral.
\end{quote}

\textbf{Parsing:} We extract the last floating-point number in the response and clamp to [$-1$, $1$].

\section{Topic Axis Definitions}
\label{app:axis_definitions}
-

\section{Complete Experimental Protocol}
\textbf{Post Generation Process:}
\begin{enumerate}
  \item For each connected agent $i$, construct prompt using agent's current opinion
  \item Include up to 6 most recent neighbor posts as context (truncated to 220 characters each)
  \item Send prompt to GPT-5 or Grok via LiteLLM
  \item Extract response text and prefix with "Agent {i}:"
  \item Store post for this timestep
\end{enumerate}

\textbf{Post Rating Process:}
\begin{enumerate}
  \item For each agent $i$ and neighbor $j$, construct rating prompt
  \item Send prompt to GPT-5 or Grok via LiteLLM
  \item Parse response using regex to extract last numeric value
  \item Clamp value to $[-1, 1]$ range
  \item Store rating in pairwise matrix $R[i,j]$
\end{enumerate}

\textbf{Opinion Update Process:}
\begin{enumerate}
  \item For each agent $i$, compute mean rating from neighbors
  \item Convert to math domain $[0,1]$: $x_i = \frac{\bar{r}_i + 1}{2}$
  \item Apply DeGroot update: $x_i^{(t+1)} = \sum_{j} w_{ij} x_j^{(t)}$
  \item Convert back to agent domain $[-1,1]$: $x_i^{(t+1)} = 2x_i^{(t+1)} - 1$
\end{enumerate}

\subsection{Complete Results with Standard Deviations}
-

\end{document}
